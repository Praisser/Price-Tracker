PROJECT: Multi-Website Price Tracker (Django + Web Scraping)

=================================================
PROJECT ARCHITECTURE
=================================================

High-Level Flow:

User (Browser)
  |
  v
Django URLs
  |
  v
Django Views
  |
  v
Scraper Engine
  |
  v
Data Normalizer
  |
  v
Price Comparator
  |
  v
Database (Cached Results)
  |
  v
Dashboard UI (HTML/CSS/JS)


-------------------------------------------------
FOLDER STRUCTURE
-------------------------------------------------

price_tracker/
│
├── core/
│   ├── views.py
│   ├── urls.py
│   ├── models.py
│   ├── services/
│   │   ├── scraper/
│   │   │   ├── base.py
│   │   │   ├── amazon.py
│   │   │   └── flipkart.py
│   │   ├── normalizer.py
│   │   └── comparator.py
│   │
│   ├── templates/
│   │   └── dashboard.html
│   │
│   └── static/
│       ├── css/
│       └── js/
│
├── price_tracker/
│   ├── settings.py
│   ├── urls.py
│
└── manage.py


=================================================
PHASE-WISE DEVELOPMENT PLAN
=================================================

-------------------------------------------------
PHASE 1: Project Setup & Base UI
-------------------------------------------------
Goal:
- Set up Django project
- Display homepage with search bar

Tasks:
- Create Django project and app
- Configure templates and static files
- Create basic dashboard UI

Outcome:
- App runs successfully
- Search bar visible
- No backend logic yet


-------------------------------------------------
PHASE 2: Database & Models
-------------------------------------------------
Goal:
- Store product searches and price data

Tasks:
- Create Product model
- Create PriceResult model
- Run migrations
- Register models in admin panel

Models:
Product:
- name
- search_query

PriceResult:
- product (ForeignKey)
- website
- price
- url
- scraped_at

Outcome:
- Database ready
- Data visible in Django admin


-------------------------------------------------
PHASE 3: Scraper Engine
-------------------------------------------------
Goal:
- Scrape product prices from websites

Tasks:
- Create base scraper class
- Write individual scrapers for each website
- Normalize scraped data

Output Format:
{
  website: "Amazon",
  price: 61999,
  url: "https://..."
}

Outcome:
- Scrapers return clean data
- Tested using Python shell


-------------------------------------------------
PHASE 4: Search Flow Integration
-------------------------------------------------
Goal:
- Connect frontend search to backend scraping

Tasks:
- Handle POST request from search bar
- Trigger scrapers
- Save results in database
- Send results to dashboard

Flow:
User Search → View → Scraper → Database → Template

Outcome:
- Search shows price comparison results


-------------------------------------------------
PHASE 5: Price Comparison Logic
-------------------------------------------------
Goal:
- Find best price automatically

Tasks:
- Sort prices
- Highlight cheapest option
- Add "Best Deal" badge

Logic:
best_price = minimum(price_list)

Outcome:
- Cheapest price clearly highlighted


-------------------------------------------------
PHASE 6: Dashboard UI Polish
-------------------------------------------------
Goal:
- Improve UI/UX

Tasks:
- Comparison table
- Buy buttons
- Responsive layout
- Error handling

Outcome:
- Professional-looking dashboard


-------------------------------------------------
PHASE 7: Caching & Performance
-------------------------------------------------
Goal:
- Avoid scraping every time

Tasks:
- Cache results for 6–12 hours
- Check database before scraping

Logic:
If cached data exists → use it
Else → scrape again

Outcome:
- Faster load times
- Reduced scraping risk


-------------------------------------------------
PHASE 8: Deployment & Documentation
-------------------------------------------------
Goal:
- Make project public and resume-ready

Tasks:
- Deploy on Render / AWS
- Write README file
- Add screenshots
- Add legal disclaimer

Outcome:
- Live website
- GitHub repository
- Resume-ready project


=================================================
END OF DOCUMENT
=================================================
